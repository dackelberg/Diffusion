{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "433f8c2e",
   "metadata": {},
   "source": [
    "**TP 4 Apprentissage multi-niveau de bruits, Unet, Mnist**\n",
    "\n",
    "\n",
    "**Exercice 1:** (conditionnement par le bruit) \n",
    "Construire un réseau de neurones `pot` (MLP) $\\mathbb{R}^3 \\rightarrow \\mathbb{R}$ et `grad` (MLP) $\\mathbb{R}^3 \\rightarrow \\mathbb{R}^2$ qui vont servir à apprendre  respectivement la densité (potentiel) et le score (son gradient) conditionnés par une valeur de bruit. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a65f7a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn        #neuronal network\n",
    "from torch import optim     #descente de gradient\n",
    "from sklearn.model_selection import train_test_split        #split into train and test set\n",
    "from torch.utils.data import DataLoader\n",
    "import itertools\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbfdd862",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = 3       #input layer with 3 neurons\n",
    "hidden_dim = 10     #hidden layer with 10 neurons\n",
    "output_dim = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "143aac3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## on ajoute un paramètre, car pot et grad sont maintenant de R^3 au lieu de R^2.\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        self.layer_1 = nn.Linear(input_dim, hidden_dim)\n",
    "        nn.init.kaiming_uniform_(self.layer_1.weight, nonlinearity=\"relu\")\n",
    "        self.layer_2 = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        #prend max(0,x)\n",
    "        x = torch.nn.functional.relu(self.layer_1(x))\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        #limite les amplitudes à 1. pour l'instant pas besoin\n",
    "        x = torch.nn.functional.sigmoid(self.layer_2(x))\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e2113c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#instantiate a model of NN class for density estimation\n",
    "model = NeuralNetwork(input_dim, hidden_dim, output_dim)\n",
    "print(model)\n",
    "\n",
    "#alpha = step-size in direction of gradient\n",
    "learning_rate = 0.1\n",
    "#loss function giving error between real and estimated data\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86ac47d1",
   "metadata": {},
   "source": [
    "**Exercice 2:** (apprentissage multi-bruit du score)\n",
    "En s'inspirant de l'exercice 6 du TP 3 et en basant sur la méthode vue en cours, apprendre à un réseau de neurones à prédire l'inverse du bruit qui est rajouté à un point des données, conditionné par le niveau de bruit appliqué. Faire une représentation du champs de vecteur associé"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6243cd85",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import cm\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from torch.distributions.multivariate_normal import MultivariateNormal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc652fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def deux_bosses(mean_1, mean_2, cov_1, cov_2, n):\n",
    "    data_1 = np.random.multivariate_normal(mean_1, cov_1, size=n//2)\n",
    "    data_2 = np.random.multivariate_normal(mean_2, cov_2, size=n//2)\n",
    "    return np.concatenate((data_1, data_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5686ca06",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = deux_bosses([0, 0], [6, 4], [[1, 0], [0, 1]], [[1, 0], [0, 1]], 20000)\n",
    "print(samples.shape)\n",
    "plt.scatter(samples[:,0], samples[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a58cef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#split dataset in train and test\n",
    "\n",
    "X_train, X_test = train_test_split(samples,test_size=.33, random_state=26)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03e4ab6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#enables to create different batches instead of treating whole dataset taking too much CPU\n",
    "\n",
    "# Convert data to torch tensors\n",
    "class Data():\n",
    "    def __init__(self, X):\n",
    "        self.X = torch.from_numpy(X.astype(np.float32))\n",
    "        self.len = self.X.shape[0]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.X[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8caaa91",
   "metadata": {},
   "outputs": [],
   "source": [
    "#set batch size for first model\n",
    "batch_size = 100\n",
    "\n",
    "# Instantiate training and test data\n",
    "train_data = Data(X_train)\n",
    "train_dataloader = DataLoader(dataset=train_data, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "test_data = Data(X_test)\n",
    "test_dataloader = DataLoader(dataset=test_data, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Check it's working\n",
    "for batch, X in enumerate(train_dataloader):\n",
    "    print(f\"Batch: {batch+1}\")\n",
    "    print(f\"X shape: {X.shape}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab9ed675",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = torch.Tensor([0,0])\n",
    "cov2 = torch.Tensor([[1, 0], [0, 1]])\n",
    "distrib = MultivariateNormal(loc=mean, covariance_matrix=cov2)\n",
    "distrib.rsample(sample_shape=torch.Size([100, 2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef4b4c39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. model pour grads\n",
    "\n",
    "input_dim_grad = 3       #input layer with 2 neurons\n",
    "hidden_dim_grad = 10     #hidden layer with 10 neurons\n",
    "output_dim_grad = 2      #output layer with 1 neuron\n",
    "\n",
    "class NeuralNetworkGrad(nn.Module):\n",
    "    def __init__(self, input_dim_grad, hidden_dim_grad, output_dim_grad):\n",
    "        super(NeuralNetworkGrad, self).__init__()\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        self.layer_1 = nn.Linear(input_dim_grad, hidden_dim_grad)\n",
    "        self.layer_2 = nn.Linear(hidden_dim_grad, output_dim_grad)\n",
    "\n",
    "    def forward(self, x):\n",
    "        #prend max(0,x)\n",
    "        x = torch.nn.functional.relu(self.layer_1(x))\n",
    "        x = self.dropout(x) #apply dropout correctly\n",
    "        #do not use the sigmoid activation function for the second layer anymore.\n",
    "        #gradients can be positive or negative -> they do not take values only between 0 and 1!\n",
    "        #sigmoid prevents the model from learning correct gradients with negative values.\n",
    "        x = self.layer_2(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "#instantiate a model of NN class\n",
    "modelGrad = NeuralNetworkGrad(input_dim_grad, hidden_dim_grad, output_dim_grad)\n",
    "print(modelGrad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c301e1a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#alpha = step-size in direction of gradient\n",
    "learning_rate = 0.01\n",
    "#loss function giving error between real and estimated data\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "optimizer_grad = torch.optim.SGD(modelGrad.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb1ef031",
   "metadata": {},
   "outputs": [],
   "source": [
    "#compute average\n",
    "sampav=np.average(samples, axis=0)\n",
    "\n",
    "#compute maximal distance between average and all points in datasets\n",
    "def maxdistance(dataset):\n",
    "    #compute average\n",
    "    average=np.average(samples, axis=0)\n",
    "    dist = np.linalg.norm(average - dataset[0])\n",
    "    for tup in range(len(dataset)):\n",
    "        disttemp =np.linalg.norm(average-dataset[tup])\n",
    "        if disttemp >dist:\n",
    "            dist = disttemp\n",
    "    return dist\n",
    "\n",
    "\n",
    "#compute sigma_t\n",
    "def sigmat(dataset,t):\n",
    "    sigmaMin=0.01\n",
    "    maxdist = maxdistance(dataset)\n",
    "    sigmaMax = 2*maxdist\n",
    "    return sigmaMin*((sigmaMax/sigmaMin)**t)\n",
    "def sample_batch(dataset,n):\n",
    "    sigmas=[]\n",
    "    times=np.random.rand(n)\n",
    "\n",
    "    for i in range(0,len(times)):\n",
    "        sigmas.append(sigmat(dataset,times[i]))\n",
    "    return torch.Tensor(sigmas)\n",
    "\n",
    "\n",
    "print(sample_batch(samples,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a18e709e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train the model\n",
    "num_epochs = 1\n",
    "loss_values_grad = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for X in train_dataloader:\n",
    "\n",
    "        # zero the parameter gradients\n",
    "\n",
    "        optimizer_grad.zero_grad()\n",
    "\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        #on sample un batch de niveau de bruit\n",
    "        batch_amplitude_bruit=sample_batch(samples,torch.Size([X.shape[0]])[0])\n",
    "        #reparametrization trick\n",
    "\n",
    "        bruit=distrib.rsample(sample_shape=torch.Size([X.shape[0]]))\n",
    "        print(bruit.shape)\n",
    "        print(batch_amplitude_bruit.shape)\n",
    "        bruit=bruit*batch_amplitude_bruit[:,None]\n",
    "\n",
    "        print(bruit.shape)\n",
    "        pred = modelGrad(X+bruit)\n",
    "        loss = loss_fn(pred,-bruit)\n",
    "        loss_values_grad.append(loss.item())\n",
    "        loss.backward()\n",
    "        optimizer_grad.step()\n",
    "\n",
    "print(\"Training Complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c4e8c7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def torch_ls(fon, p, n, epsi):\n",
    "    point = p\n",
    "    tensor = point\n",
    "    mu1=torch.Tensor([[epsi, 0],[0, epsi]])\n",
    "    mean = torch.Tensor([0,0])\n",
    "    distrib = MultivariateNormal(loc=mean, covariance_matrix=mu1)\n",
    "    for i in range(n):\n",
    "        point = point + fon(point) + distrib.rsample(sample_shape=torch.Size([point.shape[0]]))\n",
    "        print(point)\n",
    "        print(tensor)\n",
    "        tensor=torch.cat((tensor,point))\n",
    "\n",
    "    return tensor\n",
    "\n",
    "'''\n",
    "def torch_ls2(fon, n):\n",
    "    mu1=torch.Tensor([[5, 0],[0, 5]])\n",
    "    depart=torch.Tensor(np.random.multivariate_normal([2.5,2.5],mu1,n)) #l'echantilloner d'une loi normale.\n",
    "    #print(depart)\n",
    "    liste= [torch_ls(fon,depart[i],30,0.01)[-1] for i in range(depart.shape[0])]\n",
    "    return np.array(liste)\n",
    "'''\n",
    "\n",
    "liste2=torch_ls(modelGrad,torch.Tensor([[0,4],[2,3]]),100,0.01)\n",
    "print(liste2.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c28106fc",
   "metadata": {},
   "source": [
    "**Exercice 3:** (champ de vecteur en grande dimension)\n",
    "Nous souhaitons appliquer la méthode ci-dessus à des images $28\\times 28$ (dataset mnist). Nous voulons donc pour cela utiliser une architecture Unet. Construire un réseau de neurone `gradU` (Unet) $\\mathbb{R}^{28\\times 28} \\rightarrow \\mathbb{R}^{28\\times 28}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a16465a",
   "metadata": {},
   "outputs": [],
   "source": [
    "m= nn.Conv2d(1, 4, kernel_size=4, padding=0, stride=2)\n",
    "\n",
    "input = torch.randn(1,28,28)\n",
    "output = m(input)\n",
    "#output.shape\n",
    "\n",
    "m2=nn.Conv2d(4, 8, kernel_size=4, padding=0, stride=2)\n",
    "output2=m2(output)\n",
    "#output2.shape\n",
    "\n",
    "m3=nn.ConvTranspose2d(8, 4, kernel_size=4, stride=2)\n",
    "m4= nn.ConvTranspose2d(4, 1, kernel_size=4, stride=2)\n",
    "\n",
    "output3=m3(output2)\n",
    "output4=m4(output3)\n",
    "#output3.shape\n",
    "#output4.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40997193",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNet_simple(nn.Module):\n",
    "    def __init__(self, channels=[32, 64, 128, 256], embed_dim=256,\n",
    "               text_dim=256):\n",
    "        super().__init__()\n",
    "\n",
    "        # Other model properties\n",
    "        self.act = nn.SiLU()\n",
    "\n",
    "        # Encoding layers\n",
    "        self.conv1 = nn.Conv2d(1, channels[0], 3, stride=1, bias=False)\n",
    "        self.gnorm1 = nn.GroupNorm(4, num_channels=channels[0])\n",
    "\n",
    "        self.conv2 = nn.Conv2d(channels[0], channels[1], 3, stride=2, bias=False)\n",
    "\n",
    "        self.gnorm2 = nn.GroupNorm(32, num_channels=channels[1])\n",
    "\n",
    "        self.conv3 = nn.Conv2d(channels[1], channels[2], 3, stride=2, bias=False)\n",
    "        self.gnorm3 = nn.GroupNorm(32, num_channels=channels[2])\n",
    "\n",
    "\n",
    "        self.conv4 = nn.Conv2d(channels[2], channels[3], 3, stride=2, bias=False)\n",
    "        self.gnorm4 = nn.GroupNorm(32, num_channels=channels[3])\n",
    "\n",
    "\n",
    "        # Decoding layers\n",
    "        self.tconv4 = nn.ConvTranspose2d(channels[3], channels[2], 3, stride=2, bias=False)\n",
    "        self.tgnorm4 = nn.GroupNorm(32, num_channels=channels[2])\n",
    "\n",
    "        self.tconv3 = nn.ConvTranspose2d(channels[2], channels[1], 3, stride=2, bias=False, output_padding=1)\n",
    "\n",
    "        self.tgnorm3 = nn.GroupNorm(32, num_channels=channels[1])\n",
    "\n",
    "        self.tconv2 = nn.ConvTranspose2d(channels[1], channels[0], 3, stride=2, bias=False, output_padding=1)\n",
    "\n",
    "        self.tgnorm2 = nn.GroupNorm(32, num_channels=channels[0])\n",
    "\n",
    "        self.tconv1 = nn.ConvTranspose2d(channels[0], 1, 3, stride=1)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        # Encoding\n",
    "        h1 = self.act(self.gnorm1(self.conv1(x) ))\n",
    "        h2 = self.act(self.gnorm2(self.conv2(h1) ))\n",
    "        h3 = self.act(self.gnorm3(self.conv3(h2) ))\n",
    "        h4 = self.act(self.gnorm4(self.conv4(h3) ))\n",
    "\n",
    "\n",
    "        # Decoding\n",
    "        h = self.act(self.tgnorm4(self.tconv4(h4) ))\n",
    "        h = self.act(self.tgnorm3(self.tconv3(h + h3) ))\n",
    "        h = self.act(self.tgnorm2(self.tconv2(h + h2) ))\n",
    "        h = self.tconv1(h + h1)\n",
    "\n",
    "        return h\n",
    "\n",
    "modelGradU = UNet_simple()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27a922d6",
   "metadata": {},
   "source": [
    "**Exercice 4:** (estimation naive du score)\n",
    "En se basant sur la méthode vue à l'exercice 6 du TP 3, apprendre au réseau `gradU` à prédire l'inverse du bruit qui est rajouté à un point des données mnist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85206234",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "from torchvision import transforms\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
    "\n",
    "batch_size = 4\n",
    "trainset = torchvision.datasets.MNIST(root='./data/', train=True, download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "\n",
    "testset = torchvision.datasets.MNIST(root='./data/', train=False, download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size, shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d16db302",
   "metadata": {},
   "outputs": [],
   "source": [
    "for X in trainloader:\n",
    "  modelGradU(X[0])\n",
    "  print(modelGradU(X[0]).shape)\n",
    "  break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "747e720b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#alpha = step-size in direction of gradient\n",
    "learning_rate = 0.01\n",
    "#loss function giving error between real and estimated data\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "optimizer_gradU = torch.optim.SGD(modelGradU.parameters(), lr=learning_rate)\n",
    "\n",
    "meanU = torch.zeros(28*28)\n",
    "covU = torch.eye(28*28)\n",
    "distribU = MultivariateNormal(loc=meanU, covariance_matrix=covU)\n",
    "distribU.rsample(sample_shape=torch.Size([100, 28*28]))\n",
    "\n",
    "#Train the model\n",
    "num_epochs = 1\n",
    "loss_values_grad = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for X in trainloader:\n",
    "        X=X[0]\n",
    "        #print(torch.Size([X.shape[0]])[0])\n",
    "        # zero the parameter gradients\n",
    "\n",
    "        optimizer_gradU.zero_grad()\n",
    "\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        #on sample un batch de niveau de bruit\n",
    "        batch_amplitude_bruit=sample_batch(samples,torch.Size([X.shape[0]])[0])\n",
    "        #print(batch_amplitude_bruit)\n",
    "        #reparametrization trick\n",
    "\n",
    "        bruit=distrib.rsample(sample_shape=torch.Size([X.shape[0]]))\n",
    "        #print(X.shape[0])\n",
    "        #print(bruit.shape)\n",
    "        #print(batch_amplitude_bruit.shape)\n",
    "        bruit=bruit*batch_amplitude_bruit[:,None]\n",
    "\n",
    "        #print(bruit.shape)\n",
    "        #print(X.shape)\n",
    "        #print(torch.add(X,bruit.view(4,2,1,1)).shape)\n",
    "        #print(bruit)\n",
    "        #print(X)\n",
    "        #print(torch.add(X,bruit.view(4,2,1,1)))\n",
    "        pred = modelGradU(torch.add(X,bruit.view(4,2,1,1))[:, :1, :, :]) #torch.add to add bruit for every batch to all of 28x28 pixels.\n",
    "                                                                         #have 8 values in bruit, thus 2 Channels, then keep only the first Channel with [:, :1, :, :]? otw. doesn't work in UNet.\n",
    "        loss = loss_fn(pred,-bruit.view(4,2,1,1))     #need to correclty \"view\" bruit as 4,2,1,1 tensor.\n",
    "        loss_values_grad.append(loss.item())\n",
    "        loss.backward()\n",
    "        optimizer_gradU.step()\n",
    "\n",
    "print(\"Training Complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75ec276f",
   "metadata": {},
   "source": [
    "**Exercice 5:** (inférence par dynamique langevienne et visualisation)\n",
    "En s'inspirant de l'exercice 3 du TP 3, écrire une fonction `langevin_sample(fon,n)` qui prend en paramètre une fonction `fon`$:\\mathbb{R}^{28\\times 28}\\rightarrow\\mathbb{R}^{28\\times 28}$ et échantillone un élément suivant cette densité en échantillon un point de $\\mathbb{R}^{28\\times 28}$ selon une loi normale puis effectue n pas à l'aide du schéma numérique d'Euler, en ajoutant à chaque pas une petite perturbation tirée selon une loi normale. Faire une matrice avec en ligne 10 points dans la courbe intégrale d'un échantillon et en colonne les courbes intégralles de 10 échantillons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8db3f710",
   "metadata": {},
   "outputs": [],
   "source": [
    "## coder ici"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b98567a",
   "metadata": {},
   "source": [
    "**Exercice 6:** (conditionnement par le bruit) \n",
    "Construire un réseau de neurones `gradUmulti` (Unet) $\\mathbb{R}^{28\\times 28+1} \\rightarrow \\mathbb{R}^{28\\times 28}$ qui va servir à apprendre  le score conditionné par une valeur de bruit. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "957b0f3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## coder ici"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42c4c172",
   "metadata": {},
   "source": [
    "**Exercice 7:** (apprentissage multi-bruit du score)\n",
    "En s'inspirant de l'exercice 2, apprendre au réseau `gradUmulti` à prédire l'inverse du bruit qui est rajouté à un point des données, conditionné par le niveau de bruit appliqué."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e728d9a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "## coder ici"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e205f216",
   "metadata": {},
   "source": [
    "**Exercice 8:** (inférence par dynamique langevienne)\n",
    "En s'inspirant de l'exercice 5, écrire une fonction `langevin_sample_schedule(fon,n)` qui prend en paramètre une fonction `fon`$:\\mathbb{R}^{28\\times 28+1}\\rightarrow\\mathbb{R}^{28\\times 28}$ et échantillone un élément suivant la distribution des données. Pour ce faire, elle échantillonne un point de $\\mathbb{R}^{28\\times 28}$ selon une loi normale puis effectue n pas à l'aide du schéma numérique d'Euler, en ajoutant à chaque pas une petite perturbation tirée selon une loi normale, en prenant soin de de choisir le niveau de bruit en fonction d'un scheduler. Sampler et représenter 10 courbes intégrales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "91b4b488",
   "metadata": {},
   "outputs": [],
   "source": [
    "## coder ici"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
